{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Brain Tumor Classification Training - M2 MacBook\\n",
    "Optimized for Apple Silicon using MPS (Metal Performance Shaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\\n",
    "import json\\n",
    "import numpy as np\\n",
    "import torch\\n",
    "import torch.nn as nn\\n",
    "import torch.optim as optim\\n",
    "import torch.nn.functional as F\\n",
    "from torch.utils.data import DataLoader, random_split\\n",
    "from torchvision import datasets, transforms, models\\n",
    "from copy import deepcopy\\n",
    "import matplotlib.pyplot as plt\\n",
    "from pathlib import Path\\n",
    "\\n",
    "# Device selection - M2 optimized\\n",
    "if torch.backends.mps.is_available():\\n",
    "    device = torch.device(\\"mps\\")\\n",
    "    print(\\"âœ“ Using MPS (Apple Silicon GPU)\\")\\n",
    "elif torch.cuda.is_available():\\n",
    "    device = torch.device(\\"cuda\\")\\n",
    "    print(\\"âœ“ Using CUDA GPU\\")\\n",
    "else:\\n",
    "    device = torch.device(\\"cpu\\")\\n",
    "    print(\\"âš  Using CPU (consider using GPU for faster training)\\")\\n",
    "\\n",
    "print(f\\"PyTorch: {torch.__version__}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\\n",
    "CONFIG = {\\n",
    "    'data_dir': Path(__file__).parent.parent / 'data' / 'Brain_Tumor_Dataset' / 'Training',\\n",
    "    'output_dir': Path(__file__).parent.parent / 'runs',\\n",
    "    'model_dir': Path(__file__).parent.parent / 'models',\\n",
    "    'batch_size': 32,\\n",
    "    'epochs': 30,\\n",
    "    'patience': 5,\\n",
    "    'lr_backbone': 3e-4,\\n",
    "    'lr_head': 1e-3,\\n",
    "    'weight_decay': 1e-4,\\n",
    "    'val_split': 0.2,\\n",
    "    'num_workers': 0,  # 0 is often best for MPS\\n",
    "}\\n",
    "\\n",
    "# Ensure directories exist\\n",
    "CONFIG['output_dir'].mkdir(parents=True, exist_ok=True)\\n",
    "CONFIG['model_dir'].mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms - optimized\\n",
    "MEAN = [0.485, 0.456, 0.406]\\n",
    "STD = [0.229, 0.224, 0.225]\\n",
    "\\n",
    "train_tf = transforms.Compose([\\n",
    "    transforms.Grayscale(num_output_channels=3),\\n",
    "    transforms.Resize(256),\\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\\n",
    "    transforms.RandomRotation(15),\\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15),\\n",
    "    transforms.ToTensor(),\\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\\n",
    "])\\n",
    "\\n",
    "val_tf = transforms.Compose([\\n",
    "    transforms.Grayscale(num_output_channels=3),\\n",
    "    transforms.Resize(256),\\n",
    "    transforms.CenterCrop(224),\\n",
    "    transforms.ToTensor(),\\n",
    "    transforms.Normalize(mean=MEAN, std=STD),\\n",
    "])\\n",
    "\\n",
    "# Load dataset\\n",
    "full_dataset = datasets.ImageFolder(root=str(CONFIG['data_dir']), transform=train_tf)\\n",
    "class_names = full_dataset.classes\\n",
    "\\n",
    "# Split dataset\\n",
    "val_size = int(CONFIG['val_split'] * len(full_dataset))\\n",
    "train_size = len(full_dataset) - val_size\\n",
    "train_dataset, val_dataset = random_split(\\n",
    "    full_dataset, \\n",
    "    [train_size, val_size],\\n",
    "    generator=torch.Generator().manual_seed(42)  # Reproducibility\\n",
    ")\\n",
    "\\n",
    "# Apply validation transform\\n",
    "val_dataset.dataset.transform = val_tf\\n",
    "\\n",
    "# Data loaders\\n",
    "train_loader = DataLoader(\\n",
    "    train_dataset, \\n",
    "    batch_size=CONFIG['batch_size'],\\n",
    "    shuffle=True,\\n",
    "    num_workers=CONFIG['num_workers'],\\n",
    "    pin_memory=True if device.type == 'mps' else False\\n",
    ")\\n",
    "\\n",
    "val_loader = DataLoader(\\n",
    "    val_dataset,\\n",
    "    batch_size=CONFIG['batch_size'],\\n",
    "    shuffle=False,\\n",
    "    num_workers=CONFIG['num_workers'],\\n",
    "    pin_memory=True if device.type == 'mps' else False\\n",
    ")\\n",
    "\\n",
    "print(f\\"Classes: {class_names}\\")\\n",
    "print(f\\"Training: {train_size} | Validation: {val_size}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup - ResNet18\\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\\n",
    "\\n",
    "# Freeze early layers\\n",
    "for param in model.parameters():\\n",
    "    param.requires_grad = False\\n",
    "\\n",
    "# Unfreeze layer3, layer4\\n",
    "for name, param in model.named_parameters():\\n",
    "    if 'layer3' in name or 'layer4' in name:\\n",
    "        param.requires_grad = True\\n",
    "\\n",
    "# Replace classifier\\n",
    "model.fc = nn.Linear(model.fc.in_features, len(class_names))\\n",
    "\\n",
    "model = model.to(device)\\n",
    "\\n",
    "# Optimizer\\n",
    "params = [\\n",
    "    {'params': [p for    n, p in model.named_parameters() \\n",
    "                if p.requires_grad and ('layer3' in n or 'layer4' in n)],\\n",
    "     'lr': CONFIG['lr_backbone']},\\n",
    "    {'params': model.fc.parameters(), 'lr': CONFIG['lr_head']}\\n",
    "]\\n",
    "\\n",
    "optimizer = optim.AdamW(params, weight_decay=CONFIG['weight_decay'])\\n",
    "criterion = nn.CrossEntropyLoss()\\n",
    "\\n",
    "# Count trainable parameters\\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\\n",
    "total = sum(p.numel() for p in model.parameters())\\n",
    "print(f\\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\\n",
    "    \\"\\"\\"Train for one epoch\\"\\"\\"\\n",
    "    model.train()\\n",
    "    total_loss, correct, total = 0.0, 0, 0\\n",
    "    \\n",
    "    for x, y in loader:\\n",
    "        x, y = x.to(device), y.to(device)\\n",
    "        \\n",
    "        optimizer.zero_grad(set_to_none=True)  # More efficient\\n",
    "        out = model(x)\\n",
    "        loss = criterion(out, y)\\n",
    "        loss.backward()\\n",
    "        optimizer.step()\\n",
    "        \\n",
    "        total_loss += loss.item() * x.size(0)\\n",
    "        correct += (out.argmax(1) == y).sum().item()\\n",
    "        total += y.size(0)\\n",
    "    \\n",
    "    return total_loss / total, 100 * correct / total\\n",
    "\\n",
    "\\n",
    "@torch.no_grad()\\n",
    "def validate(model, loader, criterion, device):\\n",
    "    \\"\\"\\"Validate model\\"\\"\\"\\n",
    "    model.eval()\\n",
    "    total_loss, correct, total = 0.0, 0, 0\\n",
    "    \\n",
    "    for x, y in loader:\\n",
    "        x, y = x.to(device), y.to(device)\\n",
    "        out = model(x)\\n",
    "        loss = criterion(out, y)\\n",
    "        \\n",
    "        total_loss += loss.item() * x.size(0)\\n",
    "        correct += (out.argmax(1) == y).sum().item()\\n",
    "        total += y.size(0)\\n",
    "    \\n",
    "    return total_loss / total, 100 * correct / total\\n",
    "\\n",
    "\\n",
    "# Training loop\\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\\n",
    "metrics_file = CONFIG['output_dir'] / 'metrics.json'\\n",
    "model_path = CONFIG['model_dir'] / 'brain_tumor_resnet18_final.pt'\\n",
    "\\n",
    "best_val_loss = float('inf')\\n",
    "patience_counter = 0\\n",
    "\\n",
    "# Initialize metrics file\\n",
    "with open(metrics_file, 'w') as f:\\n",
    "    json.dump([], f)\\n",
    "\\n",
    "print(\\"\\\\nðŸš€ Starting training...\\\\n\\")\\n",
    "\\n",
    "for epoch in range(CONFIG['epochs']):\\n",
    "    # Train\\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\\n",
    "    \\n",
    "    # Validate\\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\\n",
    "    \\n",
    "    # Update history\\n",
    "    history['train_loss'].append(train_loss)\\n",
    "    history['val_loss'].append(val_loss)\\n",
    "    history['train_acc'].append(train_acc)\\n",
    "    history['val_acc'].append(val_acc)\\n",
    "    \\n",
    "    # Save metrics\\n",
    "    with open(metrics_file, 'r') as f:\\n",
    "        data = json.load(f)\\n",
    "    \\n",
    "    data.append({\\n",
    "        'epoch': epoch + 1,\\n",
    "        'train_loss': float(train_loss),\\n",
    "        'val_loss': float(val_loss),\\n",
    "        'train_acc': float(train_acc),\\n",
    "        'val_acc': float(val_acc)\\n",
    "    })\\n",
    "    \\n",
    "    with open(metrics_file, 'w') as f:\\n",
    "        json.dump(data, f, indent=2)\\n",
    "    \\n",
    "    # Print progress\\n",
    "    print(f\\"Epoch {epoch+1:02d}/{CONFIG['epochs']} | \\"\\n",
    "          f\\"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | \\"\\n",
    "          f\\"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%\\")\\n",
    "    \\n",
    "    # Early stopping\\n",
    "    if val_loss < best_val_loss:\\n",
    "        best_val_loss = val_loss\\n",
    "        torch.save(model.state_dict(), model_path)\\n",
    "        print(f\\"  âœ“ Model saved (val_loss: {val_loss:.4f})\\")\\n",
    "        patience_counter = 0\\n",
    "    else:\\n",
    "        patience_counter += 1\\n",
    "        if patience_counter >= CONFIG['patience']:\\n",
    "            print(f\\"\\\\nâš  Early stopping triggered after {epoch+1} epochs\\")\\n",
    "            break\\n",
    "\\n",
    "print(f\\"\\\\nâœ… Training complete! Best model saved to: {model_path}\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\\n",
    "\\n",
    "# Loss\\n",
    "ax1.plot(history['train_loss'], label='Train', linewidth=2)\\n",
    "ax1.plot(history['val_loss'], label='Validation', linewidth=2)\\n",
    "ax1.set_xlabel('Epoch')\\n",
    "ax1.set_ylabel('Loss')\\n",
    "ax1.set_title('Training & Validation Loss')\\n",
    "ax1.legend()\\n",
    "ax1.grid(True, alpha=0.3)\\n",
    "\\n",
    "# Accuracy\\n",
    "ax2.plot(history['train_acc'], label='Train', linewidth=2)\\n",
    "ax2.plot(history['val_acc'], label='Validation', linewidth=2)\\n",
    "ax2.set_xlabel('Epoch')\\n",
    "ax2.set_ylabel('Accuracy (%)')\\n",
    "ax2.set_title('Training & Validation Accuracy')\\n",
    "ax2.legend()\\n",
    "ax2.grid(True, alpha=0.3)\\n",
    "\\n",
    "plt.tight_layout()\\n",
    "plt.savefig(CONFIG['output_dir'] / 'training_history.png', dpi=150, bbox_inches='tight')\\n",
    "plt.show()\\n",
    "\\n",
    "print(f\\"\\\\nFinal Results:\\")\\n",
    "print(f\\"  Best Val Loss: {min(history['val_loss']):.4f}\\")\\n",
    "print(f\\"  Best Val Acc: {max(history['val_acc']):.2f}%\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_brain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
